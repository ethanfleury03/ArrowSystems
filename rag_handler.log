2025-10-03 14:25:58,629 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-10-03 14:26:00,635 - INFO -    - Context window: 2048
2025-10-03 14:26:00,635 - INFO -    - Max new tokens: 512
2025-10-03 14:26:00,636 - INFO -    - Temperature: 0.3
2025-10-03 14:26:00,636 - INFO -    - Device map: auto (will use GPU if available)
2025-10-03 14:26:01,154 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-10-03 14:28:21,474 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-10-03 14:28:23,545 - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 165, in handler
    initialize_models()
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 50, in initialize_models
    llm = HuggingFaceLLM(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\llms\huggingface\base.py", line 212, in __init__
    model = model or AutoModelForCausalLM.from_pretrained(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\models\auto\auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\modeling_utils.py", line 288, in _wrapper
    return func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\modeling_utils.py", line 5179, in from_pretrained
    ) = cls._load_pretrained_model(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\modeling_utils.py", line 5543, in _load_pretrained_model
    raise ValueError(
ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.
2025-10-03 14:28:23,557 - INFO - {
  "error": "Unexpected error: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.",
  "output": "I'm sorry, I encountered an error while processing your question. Please try again."
}
2025-10-03 14:32:03,747 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-10-03 14:32:05,537 - INFO -    - Context window: 2048
2025-10-03 14:32:05,538 - INFO -    - Max new tokens: 512
2025-10-03 14:32:05,538 - INFO -    - Temperature: 0.3
2025-10-03 14:32:05,538 - INFO -    - Device map: auto (will use GPU if available)
2025-10-03 14:32:18,220 - INFO - Loading all indices.
2025-10-03 14:32:19,317 - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 201, in handler
    answer = query_index(question)
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 130, in query_index
    response = query_engine.query(formatted_question)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\base\base_query_engine.py", line 44, in query
    query_result = self._query(str_or_query_bundle)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\query_engine\retriever_query_engine.py", line 197, in _query
    response = self._response_synthesizer.synthesize(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\base.py", line 235, in synthesize
    response_str = self.get_response(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\compact_and_refine.py", line 43, in get_response
    return super().get_response(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\refine.py", line 179, in get_response
    response = self._give_response_single(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\refine.py", line 241, in _give_response_single
    program(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\refine.py", line 85, in __call__
    answer = self._llm.predict(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\llms\llm.py", line 627, in predict
    response = self.complete(formatted_prompt, formatted=True)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\llms\callbacks.py", line 435, in wrapped_llm_predict
    f_return_val = f(_self, *args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\llms\huggingface\base.py", line 339, in complete
    tokens = self._model.generate(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\utils\_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\generation\utils.py", line 2539, in generate
    result = self._sample(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\generation\utils.py", line 2867, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 1070, in forward
    transformer_outputs = self.transformer(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 867, in forward
    position_embeds = self.wpe(position_ids)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\sparse.py", line 192, in forward
    return F.embedding(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\functional.py", line 2546, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-10-03 14:32:19,324 - INFO - {
  "error": "Unexpected error: index out of range in self",
  "output": "I'm sorry, I encountered an error while processing your question. Please try again."
}
2025-10-03 14:33:36,295 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-10-03 14:33:38,085 - INFO -    - Context window: 2048
2025-10-03 14:33:38,086 - INFO -    - Max new tokens: 512
2025-10-03 14:33:38,087 - INFO -    - Temperature: 0.3
2025-10-03 14:33:38,087 - INFO -    - Device map: auto (will use GPU if available)
2025-10-03 14:33:47,436 - INFO - Loading all indices.
2025-10-03 14:33:48,185 - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 197, in handler
    answer = query_index(question)
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 126, in query_index
    response = query_engine.query(formatted_question)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\base\base_query_engine.py", line 44, in query
    query_result = self._query(str_or_query_bundle)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\query_engine\retriever_query_engine.py", line 197, in _query
    response = self._response_synthesizer.synthesize(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\base.py", line 235, in synthesize
    response_str = self.get_response(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\compact_and_refine.py", line 43, in get_response
    return super().get_response(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\refine.py", line 179, in get_response
    response = self._give_response_single(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\refine.py", line 241, in _give_response_single
    program(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\response_synthesizers\refine.py", line 85, in __call__
    answer = self._llm.predict(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\llms\llm.py", line 627, in predict
    response = self.complete(formatted_prompt, formatted=True)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index_instrumentation\dispatcher.py", line 335, in wrapper
    result = func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\core\llms\callbacks.py", line 435, in wrapped_llm_predict
    f_return_val = f(_self, *args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\llms\huggingface\base.py", line 339, in complete
    tokens = self._model.generate(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\utils\_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\generation\utils.py", line 2539, in generate
    result = self._sample(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\generation\utils.py", line 2867, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 1070, in forward
    transformer_outputs = self.transformer(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 867, in forward
    position_embeds = self.wpe(position_ids)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\modules\sparse.py", line 192, in forward
    return F.embedding(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\torch\nn\functional.py", line 2546, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
2025-10-03 14:34:54,278 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-10-03 14:34:55,878 - INFO -    - Context window: 1024
2025-10-03 14:34:55,878 - INFO -    - Max new tokens: 256
2025-10-03 14:34:55,879 - INFO -    - Temperature: 0.1
2025-10-03 14:34:55,879 - INFO -    - Device map: cpu (stable)
2025-10-03 14:35:05,885 - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 161, in handler
    initialize_models()
  File "C:\Users\ethan\OneDrive\Desktop\ArrowSystemsInc\rag_app.py\handler.py", line 50, in initialize_models
    llm = HuggingFaceLLM(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\llama_index\llms\huggingface\base.py", line 212, in __init__
    model = model or AutoModelForCausalLM.from_pretrained(
  File "C:\Users\ethan\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\transformers\models\auto\auto_factory.py", line 607, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.
2025-10-03 14:35:48,756 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2025-10-03 14:35:50,287 - INFO -    - Context window: 1024
2025-10-03 14:35:50,287 - INFO -    - Max new tokens: 128
2025-10-03 14:35:50,289 - INFO -    - Temperature: 0.7
2025-10-03 14:35:50,290 - INFO -    - Device map: cpu (stable)
2025-10-03 14:35:56,274 - INFO - Loading all indices.
